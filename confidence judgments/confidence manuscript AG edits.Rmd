---
title             : "The influence of confidence on Judgments of Associate Memory"
shorttitle        : "Confident JAM"

author: 
  - name          : "Erin M. Buchanan"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "901 S. National Ave, Springfield, MO, 65897"
    email         : "erinbuchanan@missouristate.edu"
  - name          : "Amber Gillenwaters"
    affiliation   : "1"

affiliation:
  - id            : "1"
    institution   : "Missouri State University"
  
author_note: |
  Erin M. Buchanan is an Associate Professor of Psychology at Missouri State University, and Amber Gillenwaters is a masters degree candidate in Experimental Psychology at Missouri State University.
  
abstract: |
  Enter abstract here. Each new line herein must be indented, like this line.
  
keywords          : "keywords"

#bibliography      : ["r-references.bib"]

figsintext        : no
figurelist        : no
tablelist         : no
footnotelist      : no
lineno            : yes
mask              : no

class             : "man"
output            : papaja::apa6_pdf
---

```{r load_packages, include = FALSE}
library("papaja")

#load the data
master = read.csv("complete_data.csv")
```

don't run this block, I already made the data
```{r create-data, include = FALSE, eval = FALSE}
##this code shows how we combined the data
#set working directory to the data folder
temp = list.files(pattern="*.dat")
myfiles = lapply(temp, read.delim)
dataset = do.call(rbind, myfiles)

dataset$partno = with(dataset, paste(date,time,subject, sep = ""))
table(dataset$partno)

wordpairs = read.delim("finalpairs2.txt", header = F)
wordpairs$index = paste(tolower(as.character(wordpairs$V2)),
                        tolower(as.character(wordpairs$V3)), sep = ".") 

##these files can be found in our SPP or wordnorms2 github
swow = read.csv("strength.SWOW-EN.R123.csv")
swow$index = paste(tolower(as.character(swow$cue)),
                   tolower(as.character(swow$response)), sep = ".")
usf = read.delim("usf_norms.txt")
usf$index = paste(tolower(as.character(usf$CUE)),
                  tolower(as.character(usf$TARGET)), sep = ".")

library(expss)

wordpairs$fsg = vlookup(wordpairs$index, usf, "FSG", "index")
wordpairs$bsg = vlookup(wordpairs$index, usf, "BSG", "index")
wordpairs$swow = vlookup(wordpairs$index, swow, "R123.Strength", "index")

dataset$fsg = vlookup(dataset$trialcode, wordpairs, "fsg", "V1")
dataset$bsg = vlookup(dataset$trialcode, wordpairs, "bsg", "V1")
dataset$swow = vlookup(dataset$trialcode, wordpairs, "swow", "V1")  
write.csv(dataset, "complete_data.csv", row.names = F)
```


# Method

## Participants

## Material

## Procedure

## Data analysis

# Results

```{r data-screening, include = FALSE}
##remove text responses from the response column

master$response = as.numeric(as.character(master$response))

##remove those lines from the data

#nomiss = na.omit(master) - good idea but there are lots of trials with NAs because they do not have the FSG for confidence, which is ok 
nomiss = master[ !is.na(master$response) , ]

##remove practice trials

summary(nomiss$trialcode)
#noprac = subset(nomiss, trialcode!= "prac1" & trialcode!= "prac2" & trialcode!= "prac3" & trialcode!= "prac4" & trialcode!= "prac5" & trialcode!= "prac6" & trialcode!= "prac7" & trialcode!= "prac8")

##great - want to learn regex? you could also do grep, which looks for patterns
#so grep("real|conf", nomiss$trialcode) shows you which ones have real or conf codes
#going to use this one because we also want to drop those lost, article, old, and followup trials ... before you had dropped them with na.omit, but didn't want to do that because it would also omit the confidence trials
noprac = nomiss[ grep("real|conf", nomiss$trialcode) , ]
summary(noprac$trialcode)

##only screen the response column

summary(noprac$response)

#outliers - ok I've rethought this - we only want to screen the response column, which had a 0 to 10 scale now that I've relooked at it. I don't think someone can have an outlier then - maybe if they were the only one to pick 0 and everyone else picked like 9 and 10. But then they'd actually be doing the experiment right (0 is a more likely answer than 10). 

zscores = scale(noprac$response)
summary(zscores)
summary(noprac$response)
#i see that the z scores don't go over 1, but they do have negative three but that is someone picking zero which is a valid answer, so let's not mess with it.

#fix up the data
jamtrials = seq(4,119,5)
judgmentdata = noprac[noprac$trialnum %in% jamtrials, ]

confidencetrials = seq(5,120,5)
confidencedata = noprac[noprac$trialnum %in% confidencetrials , ]

judgmentdata$confidence = confidencedata$response

####assumptions from mlm hypothesis 1####
library(nlme)

screen = lme(response ~ fsg,
             data = judgmentdata, 
             method = "ML", 
             na.action = "na.omit",
             random = ~ fsg|partno,
             control = lmeControl(msMaxIter = 200))

standardized = as.data.frame(scale(screen$residuals))
standardized = standardized$fixed
fitted = scale(fitted.values(screen))

##Linearity
qqnorm(standardized); abline(0,1)

##normality
hist(standardized)

##homog and s
plot(fitted,standardized); abline(0,0); abline(v=0)

#rescale responses to 0 to 100
noprac$response = noprac$response * 10 - 5

#rescale others so all 100 scale
noprac[ , c("fsg", "bsg", "swow")] = noprac[ , c("fsg", "bsg", "swow")] * 100

#assumptions from mlm hypothesis 2

screen2 = lme(response ~ trialnum,
             data = confidencedata, 
             method = "ML", 
             na.action = "na.omit",
             random = ~ trialnum|partno,
             control = lmeControl(msMaxIter = 200))

standardized2 = as.data.frame(scale(screen2$residuals))
standardized2 = standardized2$fixed
fitted2 = scale(fitted.values(screen2))

##Linearity
qqnorm(standardized2); abline(0,1)

##normality
hist(standardized2)

##homog and s
plot(fitted,standardized2); abline(0,0); abline(v=0)

#assumptions from mlm hypothesis 3
screen3 = lme(response ~ confidence*fsg, 
             data = judgmentdata, 
             method = "ML", 
             na.action = "na.omit",
             random = ~1|partno)

standardized3 = as.data.frame(scale(screen3$residuals))
standardized3 = standardized3$fixed
fitted3 = scale(fitted.values(screen))

##Linearity
qqnorm(standardized3); abline(0,1)

##normality
hist(standardized3)

##homog and s
plot(fitted,standardized3); abline(0,0); abline(v=0)

```

Hypothesis 1: 
The JAM effect will replicate previous research.

Analyze with MLM:
Use partno as random factor
Test if trialcode needs to be second random factor
Use FSG as IV
Use response as the DV - subset the data without the confidence trials

```{r mlm-hyp1, include = FALSE}
library(nlme)

jamtrials = seq(4,119,5)
judgmentdata = master[master$trialnum %in% jamtrials, ]

#####intercept only model####
##gls = generalized least squares
##ML = maximum likelihood
model1 = gls(response ~ 1, 
             data = judgmentdata, 
             method = "ML", 
             na.action = "na.omit")
summary(model1)

####random intercept only model####
##note we switched to LME function
model2 = lme(response ~ 1, 
             data = judgmentdata, 
             method = "ML", 
             na.action = "na.omit",
             random = ~1|partno)
summary(model2)
anova(model1, model2)

####second level####
model2.1 = lme(response ~ 1, 
             data = judgmentdata, 
             method = "ML", 
             na.action = "na.omit",
             random = list(~1|partno, ~1|trialcode))
summary(model2.1)
anova(model1, model2, model2.1)

####predictor model####
model3 = lme(response ~ fsg, 
             data = judgmentdata, 
             method = "ML", 
             na.action = "na.omit",
             random = ~1|partno)
summary(model3)
anova(model1, model2, model3)

####random slopes####
model4 = lme(response ~ fsg,
             data = judgmentdata, 
             method = "ML", 
             na.action = "na.omit",
             random = ~ fsg|partno,
             control = lmeControl(msMaxIter = 200))
summary(model4)
anova(model1, model2, model3, model4)
```

Hypothesis 2:
Does confidence change within the course of the experiment?

Analyze with MLM:
Use partno as random factor
Use trialnum as the IV
Use response as the DV - subset the data to only confidence trials

```{r mlm-hyp2, include = FALSE}

confidencetrials = seq(5,120,5)
confidencedata = master[master$trialnum %in% confidencetrials , ]

#####intercept only model####
model5 = gls(response ~ 1, 
             data = confidencedata, 
             method = "ML", 
             na.action = "na.omit")
summary(model5)

####random intercept only model####
model6 = lme(response ~ 1, 
             data = confidencedata, 
             method = "ML", 
             na.action = "na.omit",
             random = ~1|partno)
summary(model6)
anova(model5, model6)

####predictor model####
model7 = lme(response ~ trialnum, 
             data = confidencedata, 
             method = "ML", 
             na.action = "na.omit",
             random = list(~1|partno))
summary(model7)
anova(model5, model6, model7)

####random slopes####
model8 = lme(response ~ trialnum,
             data = confidencedata, 
             method = "ML", 
             na.action = "na.omit",
             random = ~ trialnum|partno,
             control = lmeControl(msMaxIter = 200))
summary(model8)
anova(model5, model6, model7, model8)
```

Hypothesis 3:
Does confidence relate to the actual JAM? 
- Figure out the difference score between FSG and response (level of accuracy) for the trial before confidence 
- Use confidence to predict that score

Analyze with MLM:
Use partno as random factor
Use confidence*fsg as the IV
Use response as the DV 

```{r mlm-hyp3, include = FALSE}

judgmentdata$confidence = confidencedata$response

#####intercept only model####
##gls = generalized least squares
##ML = maximum likelihood
model9 = gls(response ~ 1, 
             data = judgmentdata, 
             method = "ML", 
             na.action = "na.omit")
summary(model9)

####random intercept only model####
##note we switched to LME function
model10 = lme(response ~ 1, 
             data = judgmentdata, 
             method = "ML", 
             na.action = "na.omit",
             random = ~1|partno)
summary(model10)
anova(model9, model10)

####predictor model####
model11 = lme(response ~ confidence*fsg, 
             data = judgmentdata, 
             method = "ML", 
             na.action = "na.omit",
             random = ~1|partno)
summary(model11)
anova(model9, model10, model11)

####random slopes####
model12 = lme(response ~ confidence*fsg,
             data = judgmentdata, 
             method = "ML", 
             na.action = "na.omit",
             random = ~ confidence|partno,
             control = lmeControl(msMaxIter = 200))
summary(model12)
anova(model9, model10, model11, model12)

####assumptions from mlm####
screen = lme(latency ~ response,
             data = noprac, 
             method = "ML", 
             na.action = "na.omit",
             random = ~ response|partno,
             control = lmeControl(msMaxIter = 200))
```

First, text responses were recoded to numerals. 157 observations missing data in the response section were removed from analysis. Next, practice trial responses were removed from the dataset, resulting in 5536 observed responses. The data were then divided into JAM trials and confidence trials using subsetting. In the final model, confidence trials were analyzed with JAM trials. 

Because each participant contributed a multitude of data points, a mixed-model was used to evaluate the influence of confidence on judgments of memories, while controlling for correlated error. The nlme package developed by Pinheiro, Bates, DebRoy, Sarkar, and the R Core Team (2018) was used to conduct a maximum likelihood multi-level model. First, the judgements of associative memory were expected to replicate previous research. Next, changes in confidence over the course of the experiment were examined. Lastly, the relationship between confidence and judgements of associative memories were explored. 

Participant was used as a random factor in all three hypothesis tests, and all models improved with random intercepts, providing support for the use of multi-level modeling. The predictive model for the first hypothesis included forward word strength in the judgment trials (See Table 1 for associated figures). Trial code was tested at this stage, but was not significant, suggesting that the observed effect did not differ across trials. Random slopes slightly increased the accuracy of the model. As expected, forward strength appeared to predict judgment associative memory, independent of confidence trials, *b* = 4.34, *t*(1104) = 14.42, *p* < .001.

Interestingly, confidence changed over the course of the experiment. The predictive model demonstrated that the number of the trial influenced level of confidence, *b* = 6.95, *t*(1104) = 32.40, *p* < .001. In this case, random slopes did not increase accuracy of the prediction. Finally, the relationship between confidence, JAM and word forward strength were analyzed together. Confidence contributed to the difference score between forward strength and JAM scores. There was a significant main effect between confidence and judgment associative memory, *b* = .37, *t*(1102) = 8.66, *p* < .001, as well as between forward strength and judgment associative memory, *b* = 2.05, *t*(1102) = 2.75, *p* = .006. The interaction of confidence and forward strength was not significant, *b* = .17, *t*(1102) = 1.80, *p* = .07. Again, random slopes did not increase accuracy. 

Table 1
*Multillevel model statistic information for confidence and judgment associative memory*
```{r tab, include = FALSE}

library(knitr)
tableprint = matrix(NA, nrow = 10, ncol = 8)
colnames(tableprint) = c("Hypothesis", "Model", "df", "AIC", 
                         "BIC", "chi-square", "chi-square change", "p")

tableprint[1, ] = c("1", "Intercept Only", 2, 5453.64,  5463.75, -2724.82, "n/a", "n/a")
tableprint[2, ] = c("1", "Random Intercept", 3, 5380.52, 5395.68, -2687.26, 37.56, "<.001")
tableprint[3, ] = c("1", "Predictive Variable", 4, 4989.65, 5009.87, -2490.82, 196.43, "<.001")
tableprint[4, ] = c("1", "Random Slopes", 6, 4956.05, 4986.38, -2472.03, 18.79, "<.001")
tableprint[5, ] = c("2", "Intercept Only", 2, 5070.31,  5080.41, -2433.15, "n/a", "n/a")
tableprint[6, ] = c("2", "Random Intercept", 3, 4751.223, 4766.39, -2372.61, 60.54, "<.001")
tableprint[7, ] = c("2", "Predictive Variable", 4, 4745.70, 4765.92, -2368.85, 3.76, .006)
tableprint[8, ] = c("3", "Intercept Only", 2, 5453.64, 5463.75, -2724.82, "n/a", "n/a")
tableprint[9, ] = c("3", "Random Intercept", 3, 5380.52, 5395.68, -2687.26, 37.56, "<.001")
tableprint[10, ] = c("3", "Predictive Variable", 6, 4835.14, 4865.46, -2411.57, 275.69, "<.001")
```

```{r tab, include = TRUE}
kable(tableprint)
```
*Note.* AIC: Aikaike Information Criterion, BIC: Bayesian Information Criterion

# Discussion


\newpage

# References

  @Manual{,
    title = {{nlme}: Linear and Nonlinear Mixed Effects Models},
    author = {Jose Pinheiro and Douglas Bates and Saikat DebRoy and Deepayan Sarkar and {R Core Team}},
    year = {2018},
    note = {R package version 3.1-137},
    url = {https://CRAN.R-project.org/package=nlme},
  }

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id = "refs"></div>
\endgroup
